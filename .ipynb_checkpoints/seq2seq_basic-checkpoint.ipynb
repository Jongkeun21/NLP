{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from pprint import pprint\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [['I', 'feel', 'hungry'],\n",
    "          ['tensorflow', 'is', 'very', 'difficult'],\n",
    "          ['tensorflow', 'is', 'a', 'framework', 'for', 'deep', 'learning'],\n",
    "          ['tensorflow', 'is', 'very', 'fast', 'changing']]\n",
    "targets = [['나는', '배가', '고프다'],\n",
    "          ['텐서플로우는', '매우', '어렵다'],\n",
    "          ['텐서플로우는', '딥러닝을', '위한', '프레임워크이다'],\n",
    "          ['텐서플로우는', '매우', '빠르게', '변화한다']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_vocab = list(set(sum(sources, [])))\n",
    "s_vocab.sort()\n",
    "s_vocab = ['<pad>'] + s_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source2idx = {word: idx for idx, word in enumerate(s_vocab)}\n",
    "idx2source = {idx: word for idx, word in enumerate(s_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0,\n",
      " 'I': 1,\n",
      " 'a': 2,\n",
      " 'changing': 3,\n",
      " 'deep': 4,\n",
      " 'difficult': 5,\n",
      " 'fast': 6,\n",
      " 'feel': 7,\n",
      " 'for': 8,\n",
      " 'framework': 9,\n",
      " 'hungry': 10,\n",
      " 'is': 11,\n",
      " 'learning': 12,\n",
      " 'tensorflow': 13,\n",
      " 'very': 14}\n"
     ]
    }
   ],
   "source": [
    "pprint(source2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>',\n",
      " 1: 'I',\n",
      " 2: 'a',\n",
      " 3: 'changing',\n",
      " 4: 'deep',\n",
      " 5: 'difficult',\n",
      " 6: 'fast',\n",
      " 7: 'feel',\n",
      " 8: 'for',\n",
      " 9: 'framework',\n",
      " 10: 'hungry',\n",
      " 11: 'is',\n",
      " 12: 'learning',\n",
      " 13: 'tensorflow',\n",
      " 14: 'very'}\n"
     ]
    }
   ],
   "source": [
    "pprint(idx2source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vocab = list(set(sum(targets, [])))\n",
    "t_vocab.sort()\n",
    "t_vocab = ['<pad>', '<bos>', '<eos>'] + t_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2idx = {word: idx for idx, word in enumerate(t_vocab)}\n",
    "idx2target = {idx: word for idx, word in enumerate(t_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<bos>': 1,\n",
      " '<eos>': 2,\n",
      " '<pad>': 0,\n",
      " '고프다': 3,\n",
      " '나는': 4,\n",
      " '딥러닝을': 5,\n",
      " '매우': 6,\n",
      " '배가': 7,\n",
      " '변화한다': 8,\n",
      " '빠르게': 9,\n",
      " '어렵다': 10,\n",
      " '위한': 11,\n",
      " '텐서플로우는': 12,\n",
      " '프레임워크이다': 13}\n"
     ]
    }
   ],
   "source": [
    "pprint(target2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>',\n",
      " 1: '<bos>',\n",
      " 2: '<eos>',\n",
      " 3: '고프다',\n",
      " 4: '나는',\n",
      " 5: '딥러닝을',\n",
      " 6: '매우',\n",
      " 7: '배가',\n",
      " 8: '변화한다',\n",
      " 9: '빠르게',\n",
      " 10: '어렵다',\n",
      " 11: '위한',\n",
      " 12: '텐서플로우는',\n",
      " 13: '프레임워크이다'}\n"
     ]
    }
   ],
   "source": [
    "pprint(idx2target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sequences, max_len, dic, mode='source') :\n",
    "    assert mode in ['source', 'target'], 'source와 target 중에 선택해주세요.'\n",
    "    \n",
    "    if mode == 'source' :\n",
    "        #ENCODER\n",
    "        s_input = list(map(lambda sentence: [dic.get(token) for token in sentence], sequences))\n",
    "        s_len = list(map(lambda sentence: len(sentence), s_input))\n",
    "        s_input = pad_sequences(sequences=s_input, maxlen=max_len, padding='post', truncating='post')\n",
    "        \n",
    "        return s_len, s_input\n",
    "    \n",
    "    elif mode == 'target' :\n",
    "        #DECODER\n",
    "        t_input = list(map(lambda sentence: ['<bos>']+sentence+['<eos>'], sequences))\n",
    "        t_input = list(map(lambda sentence: [dic.get(token) for token in sentence], t_input))\n",
    "        t_len = list(map(lambda sentence: len(sentence), t_input))\n",
    "        t_input = pad_sequences(sequences=t_input, maxlen=max_len, padding='post', truncating='post')\n",
    "        \n",
    "        t_output = list(map(lambda sentence: sentence+['<eos>'], sequences))\n",
    "        t_output = list(map(lambda sentence: [dic.get(token) for token in sentence], t_output))\n",
    "        t_output = pad_sequences(sequences=t_output, maxlen=max_len, padding='post', truncating='post')\n",
    "        \n",
    "        return t_len, t_input, t_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_max_len = 10\n",
    "s_len, s_input = preprocess(sequences=sources, max_len=s_max_len, dic=source2idx, mode='source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 7, 5]\n",
      "array([[ 1,  7, 10,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [13, 11, 14,  5,  0,  0,  0,  0,  0,  0],\n",
      "       [13, 11,  2,  9,  8,  4, 12,  0,  0,  0],\n",
      "       [13, 11, 14,  6,  3,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "pprint(s_len)\n",
    "pprint(s_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_max_len = 12\n",
    "t_len, t_input, t_output = preprocess(sequences=targets, max_len=t_max_len, dic=target2idx, mode='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 5, 6, 6]\n",
      "array([[ 1,  4,  7,  3,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 12,  6, 10,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 12,  5, 11, 13,  2,  0,  0,  0,  0,  0,  0],\n",
      "       [ 1, 12,  6,  9,  8,  2,  0,  0,  0,  0,  0,  0]])\n",
      "array([[ 4,  7,  3,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [12,  6, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [12,  5, 11, 13,  2,  0,  0,  0,  0,  0,  0,  0],\n",
      "       [12,  6,  9,  8,  2,  0,  0,  0,  0,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "pprint(t_len)\n",
    "pprint(t_input)\n",
    "pprint(t_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 4\n",
    "learning_rate = 5e-3\n",
    "total_step = epochs/batch_size\n",
    "buffer_size = 100\n",
    "n_batch = buffer_size//batch_size\n",
    "embedding_dim = 32\n",
    "units = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.data.Dataset.from_tensor_slices((s_len, s_input, t_len, t_input, t_output))\n",
    "data = data.shuffle(buffer_size=buffer_size)\n",
    "data = data.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units) :\n",
    "    return tf.keras.layers.CuDNNGRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model) :\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size) :\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden) :\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self) :\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model) :\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size) :\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output) :\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state\n",
    "    \n",
    "    def initialize_hidden_state(self) :\n",
    "        return tf.zeros((self.batch_size, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Encoder object at 0x000001473123F828>\n",
      "<__main__.Decoder object at 0x000001473123F898>\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(len(source2idx), embedding_dim, units, batch_size)\n",
    "decoder = Decoder(len(target2idx), embedding_dim, units, batch_size)\n",
    "pprint(encoder)\n",
    "pprint(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred) :\n",
    "    mask = 1-np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0521 23:03:17.630368 33380 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = './data_out/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "\n",
    "summary_writer = tf.contrib.summary.create_file_writer(logdir=checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10,  Loss: 0.0387,  Batch Loss: 0.9671\n",
      "Epoch: 20,  Loss: 0.0373,  Batch Loss: 0.9316\n",
      "Epoch: 30,  Loss: 0.0346,  Batch Loss: 0.8657\n",
      "Epoch: 40,  Loss: 0.0304,  Batch Loss: 0.7599\n",
      "Epoch: 50,  Loss: 0.0269,  Batch Loss: 0.6713\n",
      "Epoch: 60,  Loss: 0.0236,  Batch Loss: 0.5907\n",
      "Epoch: 70,  Loss: 0.0207,  Batch Loss: 0.5174\n",
      "Epoch: 80,  Loss: 0.0179,  Batch Loss: 0.4485\n",
      "Epoch: 90,  Loss: 0.0154,  Batch Loss: 0.3847\n",
      "Epoch: 100,  Loss: 0.0131,  Batch Loss: 0.3271\n",
      "Epoch: 110,  Loss: 0.0111,  Batch Loss: 0.2770\n",
      "Epoch: 120,  Loss: 0.0094,  Batch Loss: 0.2353\n",
      "Epoch: 130,  Loss: 0.0080,  Batch Loss: 0.2012\n",
      "Epoch: 140,  Loss: 0.0070,  Batch Loss: 0.1743\n",
      "Epoch: 150,  Loss: 0.0061,  Batch Loss: 0.1530\n",
      "Epoch: 160,  Loss: 0.0054,  Batch Loss: 0.1358\n",
      "Epoch: 170,  Loss: 0.0049,  Batch Loss: 0.1218\n",
      "Epoch: 180,  Loss: 0.0044,  Batch Loss: 0.1104\n",
      "Epoch: 190,  Loss: 0.0041,  Batch Loss: 0.1017\n",
      "Epoch: 200,  Loss: 0.0038,  Batch Loss: 0.0953\n",
      "Epoch: 210,  Loss: 0.0036,  Batch Loss: 0.0907\n",
      "Epoch: 220,  Loss: 0.0035,  Batch Loss: 0.0873\n",
      "Epoch: 230,  Loss: 0.0034,  Batch Loss: 0.0848\n",
      "Epoch: 240,  Loss: 0.0033,  Batch Loss: 0.0828\n",
      "Epoch: 250,  Loss: 0.0032,  Batch Loss: 0.0812\n",
      "Epoch: 260,  Loss: 0.0032,  Batch Loss: 0.0799\n",
      "Epoch: 270,  Loss: 0.0032,  Batch Loss: 0.0788\n",
      "Epoch: 280,  Loss: 0.0031,  Batch Loss: 0.0778\n",
      "Epoch: 290,  Loss: 0.0031,  Batch Loss: 0.0770\n",
      "Epoch: 300,  Loss: 0.0031,  Batch Loss: 0.0763\n",
      "Epoch: 310,  Loss: 0.0030,  Batch Loss: 0.0756\n",
      "Epoch: 320,  Loss: 0.0030,  Batch Loss: 0.0750\n",
      "Epoch: 330,  Loss: 0.0030,  Batch Loss: 0.0744\n",
      "Epoch: 340,  Loss: 0.0029,  Batch Loss: 0.0736\n",
      "Epoch: 350,  Loss: 0.0029,  Batch Loss: 0.0726\n",
      "Epoch: 360,  Loss: 0.0028,  Batch Loss: 0.0709\n",
      "Epoch: 370,  Loss: 0.0027,  Batch Loss: 0.0673\n",
      "Epoch: 380,  Loss: 0.0024,  Batch Loss: 0.0598\n",
      "Epoch: 390,  Loss: 0.0020,  Batch Loss: 0.0502\n"
     ]
    }
   ],
   "source": [
    "result = float('inf')\n",
    "epoch = 0\n",
    "\n",
    "while result > 0.05 :\n",
    "    epoch += 1\n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (s_len, s_input, t_len, t_input, t_output) in enumerate(data) :\n",
    "        loss = 0\n",
    "\n",
    "        with tf.GradientTape() as tape :\n",
    "            enc_output, enc_hidden = encoder(s_input, hidden)\n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = tf.expand_dims([target2idx['<bos>']]*batch_size, 1)\n",
    "\n",
    "            for t in range(1, t_input.shape[1]) :\n",
    "                predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "                loss += loss_function(t_input[:, t], predictions)\n",
    "                dec_input = tf.expand_dims(t_input[:, t], 1)\n",
    "\n",
    "        batch_loss = (loss/int(t_input.shape[1]))\n",
    "        result = batch_loss.numpy()\n",
    "        total_loss += batch_loss\n",
    "        variables = encoder.variables+decoder.variables\n",
    "        gradient = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradient, variables))\n",
    "\n",
    "    if epoch%10 == 0 :\n",
    "        print('Epoch: {},  Loss: {:.4f},  Batch Loss: {:.4f}'.format(epoch, total_loss/n_batch, batch_loss.numpy()))\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x14758cb1860>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(sentence, encoder, decoder, inp_lang, tar_lang, max_length_inp, max_length_tar) :\n",
    "    result = ''\n",
    "    \n",
    "    inputs = [inp_lang[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "        \n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tar_lang['<bos>']], 0)\n",
    "    \n",
    "    for t in range(max_length_tar) :\n",
    "        predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        result += idx2target[predicted_id]+' '\n",
    "        \n",
    "        if idx2target[predicted_id] == '<eos>' :\n",
    "            return result, sentence\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'tensorflow is very difficult'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result, output_sentence = prediction(sentence, encoder, decoder, source2idx, target2idx, s_max_len, t_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'텐서플로우는 매우 어렵다 <eos> '\n"
     ]
    }
   ],
   "source": [
    "pprint(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
