{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import enum\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from configs import DEFINES\n",
    "from tqdm import tqdm\n",
    "\n",
    "PAD_MASK = 0\n",
    "NON_PAD_MASK = 1\n",
    "\n",
    "FILTERS = \"([~.,!?\\\"':;)(])\"\n",
    "PAD = \"<PADDING>\"\n",
    "STD = \"<START>\"\n",
    "END = \"<END>\"\n",
    "UNK = \"<UNKNOWN>\"\n",
    "\n",
    "PAD_INDEX = 0\n",
    "STD_INDEX = 1\n",
    "END_INDEX = 2\n",
    "UNK_INDEX = 3\n",
    "\n",
    "MARKER = [PAD, STD, END, UNK]\n",
    "CHANGE_FILTER = re.compile(FILTERS)\n",
    "\n",
    "def load_data() :\n",
    "    data_df = pd.read_csv(DEFINES.data_path, header=0)\n",
    "    question, answer = list(data_df['Q']), data_df['A']\n",
    "    train_input, eval_input, train_label, eval_label = train_test_split(question, answer, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return train_input, eval_input, train_label, eval_label\n",
    "\n",
    "def prepro_like_morphlized(data) :\n",
    "    morph_analyzer = Okt()\n",
    "    result_data = list()\n",
    "    \n",
    "    for seq in tqdm(data) :\n",
    "        # \" \".join -> 리스트의 인덱스를 띄어쓰기로 구분하여 붙임 \n",
    "        # example -- a = [1, 2, 3], \" \".join(a) => 1 2 3\n",
    "        morphlized_seq = \" \".join(morph_analyzer.morphs(seq.replace(' ', '')))\n",
    "        result_data.append(morphlized_seq)\n",
    "        \n",
    "    return result_data\n",
    "\n",
    "def enc_processing(value, dictionary) :\n",
    "    sequences_input_index = []\n",
    "    sequences_length = []\n",
    "    \n",
    "    if DEFINES.tokenize_as_morph :\n",
    "        value = prepro_like_morphlized(vale)\n",
    "        \n",
    "    for sequence in value :\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = []\n",
    "        \n",
    "        for word in sequence.split() :\n",
    "            if dictionary.get(word) is not None :\n",
    "                sequence_index.extend([dictionary[word]])\n",
    "            else :\n",
    "                sequence_index.extend([dictionary[UNK]])\n",
    "        \n",
    "        if len(sequence_index) > DEFINES.max_seq_length :\n",
    "            sequence_index = sequence_index[:DEFINES.max_seq_length]\n",
    "            \n",
    "        sequences_length.append(len(sequence_index))\n",
    "        sequence_index += (DEFINES.max_seq_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequence_index.reverse()\n",
    "        sequences_input_index.append(sequence_index)\n",
    "        \n",
    "    return np.asarray(sequences_input_index), sequences_length\n",
    "\n",
    "def dec_target_processing(value, dictionary) :\n",
    "    sequences_target_index = []\n",
    "    sequences_length = []\n",
    "    \n",
    "    if DEFINES.tokenize_as_morph :\n",
    "        value = prepro_like_morphlized(value)\n",
    "        \n",
    "    for sequence in value :\n",
    "        sequence = re.sub(CHANGE_FILTER, \"\", sequence)\n",
    "        sequence_index = [dictionary[word] for word in sequence.split()]\n",
    "        \n",
    "        if len(sequence_index) >= DEFINES.max_seq_length :\n",
    "            sequence_index = sequence_index[:DEFINES.max_seq_length-1] + [dictionary[END]]\n",
    "        else :\n",
    "            sequence_index += [dictionary[END]]\n",
    "            \n",
    "        sequences_length.append([PAD_MASK if num > len(sequence_index) else NON_PAD_MASK for num in range(DEFINES.max_seq_length)])\n",
    "        sequence_index += (DEFINES.max_seq_length - len(sequence_index)) * [dictionary[PAD]]\n",
    "        sequences_target_index.append(sequence_index)\n",
    "        \n",
    "    return np.asarray(sequences_target_index), np.asarray(sequences_length)\n",
    "\n",
    "def pred2string(value, dictionary) :\n",
    "    sentence_string = []\n",
    "    \n",
    "    if DEFINES.serving == True :\n",
    "        for v in value['output'] :\n",
    "            sentence_string = [dictionary[index] for index in v]\n",
    "    else :\n",
    "        for v in value :\n",
    "            sentence_string = [dictionary[index] for index in v['indexs']]\n",
    "            \n",
    "    print(\"sentence_string: \", sentence_string)\n",
    "    \n",
    "    answer = \"\"\n",
    "    \n",
    "    for word in sentence_string :\n",
    "        if word not in PAD and word not in END :\n",
    "            answer += word\n",
    "            answer += \" \"\n",
    "            \n",
    "    print(\"answer: \", answer)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def rearrange(input, target) :\n",
    "    features = {\"input\": input}\n",
    "    \n",
    "    return features, target\n",
    "\n",
    "def train_rearrange(input, length, target) :\n",
    "    features = {\"input\": input, \"length\": length}\n",
    "    \n",
    "    return features, target\n",
    "\n",
    "def train_input_fn(train_input_enc, train_target_dec_length, train_target_dec, batch_size) :\n",
    "    #tf.data.Dataset.from_tensor_slices(a, b) -- a와 b의 내용을 하나씩 자름\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((train_input_enc, train_target_dec_length, train_target_dec))\n",
    "    dataset = dataset.shuffle(buffer_size=len(train_input_enc))\n",
    "    \n",
    "    assert batch_size is not None, \"train batchSize must not be None\"\n",
    "    \n",
    "    #batch(batch_size) -- dataset을 batch_size의 크기로 구분\n",
    "    #map(func) -- func를 거친 data로 변환\n",
    "    #repeat(cnt) -- data를 cnt의 횟수만큼 반복하여 저장\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(train_rearrange)\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn(eval_input_enc, eval_target_dec, batch_size) :\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eval_input_enc, eval_taget_dec))\n",
    "    dataset = dataset.shuffle(buffer_size=len(eval_input_enc))\n",
    "    \n",
    "    assert batch_size is not None, \"eval batchSize must not be None\"\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(rearrange)\n",
    "    dataset = dataset.repeat(1)\n",
    "    \n",
    "    iterator = dataset.make_ont_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def data_tokenizer(data) :\n",
    "    words = []\n",
    "    \n",
    "    for sentence in data :\n",
    "        sentence = re.sub(CHANGE_FILTER, \"\", sentence)\n",
    "\n",
    "        for word in sentence.split() :\n",
    "            words.append(word)\n",
    "            \n",
    "    # word for word in words if word -- words 안의 index인 word가 True이면 word 반환(반복)\n",
    "    return [word for word in words if word]    \n",
    "\n",
    "def load_vocabulary() :\n",
    "    vocabulary_list = []\n",
    "    \n",
    "    if (not (os.path.exists(DEFINES.vocab_path))) :\n",
    "        if (os.path.exists(DEFINES.data_path)) :\n",
    "            data_df = pd.read_csv(DEFINES.data_path, encoding='utf-8')\n",
    "            question, answer = list(data_df['Q']), list(data_df['A'])\n",
    "            \n",
    "            if DEFINES.tokenize_as_morph :\n",
    "                question = prepro_like_morphlized(question)\n",
    "                answer = prepro_like_morphlized(answer)\n",
    "                \n",
    "            data = []\n",
    "            data.extend(question)\n",
    "            data.extend(answer)\n",
    "            \n",
    "            words = data_tokenizer(data)\n",
    "            words = list(set(words))\n",
    "            #[:0] -- 리스트 맨 앞에 '추가'할 때 사용하면 유용해\n",
    "            words[:0] = MARKER\n",
    "            \n",
    "        with open(DEFINES.vocab_path, 'w', encoding='utf-8') as vocabulary_file :\n",
    "            for word in words :\n",
    "                vocabulary_file.write(word + '\\n')\n",
    "                \n",
    "    with open(DEFINES.vocab_path, 'r', encoding='utf-8') as vocabulary_file :\n",
    "        for line in vocabulary_file :\n",
    "            #strip -- 문장 양쪽 끝의 공백 삭제(\\n 포함)\n",
    "            vocabulary_list.append(line.strip())\n",
    "            \n",
    "    char2idx, idx2char = make_vocabulary(vocabulary_list)\n",
    "    \n",
    "    return char2idx, idx2char, len(char2idx)\n",
    "\n",
    "def make_vocabulary(vocabulary_list) :\n",
    "    char2idx = {char: idx for idx, char in enumerate(vocabulary_list)}\n",
    "    idx2char = {idx: char for idx, char in enumerate(vocabulary_list)}\n",
    "    \n",
    "    return char2idx, idx2char\n",
    "\n",
    "def main(self) :\n",
    "    char2idx, idx2char, vocabulary_length = load_vocabulary()\n",
    "\n",
    "if __name__ == '__main' :\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    tf.app.run(main)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
